{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46c61bd",
   "metadata": {},
   "source": [
    "# TensorFlow Lite Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4696b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e93daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = keras.models.load_model('xception_v5_34_0.897.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e649f6a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-11-15 13:02:18--  http://bit.ly/mlbookcamp-pants\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://raw.githubusercontent.com/alexeygrigorev/clothing-dataset-small/master/test/pants/4aabd82c-82e1-4181-a84d-d0c6e550d26d.jpg [following]\n",
      "--2022-11-15 13:02:18--  https://raw.githubusercontent.com/alexeygrigorev/clothing-dataset-small/master/test/pants/4aabd82c-82e1-4181-a84d-d0c6e550d26d.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23048 (23K) [image/jpeg]\n",
      "Saving to: 'pants.jpg'\n",
      "\n",
      "     0K .......... .......... ..                              100%  998K=0.02s\n",
      "\n",
      "2022-11-15 13:02:21 (998 KB/s) - 'pants.jpg' saved [23048/23048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download image for prediction\n",
    "!wget http://bit.ly/mlbookcamp-pants -O pants.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50342a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to load image and preprocessing\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.xception import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31a6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image with expect image size\n",
    "img = load_img('pants.jpg', target_size=(299, 299))\n",
    "\n",
    "# Convert image to numpy array and add batch dimension\n",
    "x = np.array(img)\n",
    "X = np.array([x])\n",
    "\n",
    "# Preprocess image\n",
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0477da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.4526634, -7.497402 , -4.0071654, -4.0869155, 11.95938  ,\n",
       "       -3.2584846, -4.643536 ,  1.930926 , -5.294927 , -4.093612 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make model prediction on preprocessed image\n",
    "preds = model.predict(X)[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14087b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -4.4526634,\n",
       " 'hat': -7.497402,\n",
       " 'longsleeve': -4.0071654,\n",
       " 'outwear': -4.0869155,\n",
       " 'pants': 11.95938,\n",
       " 'shirt': -3.2584846,\n",
       " 'shoes': -4.643536,\n",
       " 'shorts': 1.930926,\n",
       " 'skirt': -5.294927,\n",
       " 't-shirt': -4.093612}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to classes the model was trained to predict\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "# Check the prediction scores\n",
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164f547",
   "metadata": {},
   "source": [
    "## Convert Keras Model to TF-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da8bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\awon\\AppData\\Local\\Temp\\tmpamzg4ke4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\awon\\AppData\\Local\\Temp\\tmpamzg4ke4\\assets\n"
     ]
    }
   ],
   "source": [
    "# Initial tf-lite converter for keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Make conversion\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model in tflite format\n",
    "with open('clothing-model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9e797",
   "metadata": {},
   "source": [
    "It changes the keras model into TensorFlow save model and then save it into tf-lite format. That's why we got the warnings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735b8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 163M\n",
      "-rw-r--r-- 1 awon 197121 81M Nov 15 13:03 clothing-model.tflite\n",
      "-rw-r--r-- 1 awon 197121 23K Nov 15 13:02 pants.jpg\n",
      "-rw-r--r-- 1 awon 197121 12K Nov 15 13:02 tensorflow-lite-model.ipynb\n",
      "-rw-r--r-- 1 awon 197121 83M Nov 12 23:07 xception_v5_34_0.897.h5\n"
     ]
    }
   ],
   "source": [
    "# Check the saved model and its size\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9884c7",
   "metadata": {},
   "source": [
    "Let's load the tf-lite model and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60471e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow lite module\n",
    "import tensorflow.lite as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716d9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model using Interpreter class\n",
    "interpreter = tflite.Interpreter(model_path='clothing-model.tflite')\n",
    "# Load the weights from the model to memory\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input index from interpreter\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "# Get the output index from interpreter\n",
    "output_index = interpreter.get_output_details()[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0db9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.4526663, -7.4974055, -4.0071616, -4.0869083, 11.95938  ,\n",
       "       -3.2584872, -4.643537 ,  1.930927 , -5.2949286, -4.09361  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the value of the input tensor using input_index to the image X\n",
    "interpreter.set_tensor(input_index, X)\n",
    "# Invoke the interpreter\n",
    "interpreter.invoke()\n",
    "# Get the value of the output tensor using output_index to make prediction\n",
    "preds = interpreter.get_tensor(output_index)[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c19a2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -4.4526663,\n",
       " 'hat': -7.4974055,\n",
       " 'longsleeve': -4.0071616,\n",
       " 'outwear': -4.0869083,\n",
       " 'pants': 11.95938,\n",
       " 'shirt': -3.2584872,\n",
       " 'shoes': -4.643537,\n",
       " 'shorts': 1.930927,\n",
       " 'skirt': -5.2949286,\n",
       " 't-shirt': -4.09361}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the prediction scores\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07109e1c",
   "metadata": {},
   "source": [
    "We get the same predictions as before but this time with the tensorflow lite model\n",
    "\n",
    "## Removing TF Dependencies\n",
    "\n",
    "We still have few problems with the work we have done when converting the model to tensorflow lite, we still need the `load_img` and `preprocess_input` functions from tensorflow to make the image ready for model predictions and we want to avoid using tensorflow for deployement.\n",
    "\n",
    "Therefore, we want to load image using `PIL` and to create custom function for image preprocessing.\n",
    "\n",
    "**Reference**:\n",
    "\n",
    "- [Keras utils to load image using PIL](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/utils.py)\n",
    "- [Image preprocessing](https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b65c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Image class from PIL\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9203f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image using Image class\n",
    "with Image.open('pants.jpg') as img:\n",
    "    # Expected image size and resampling filter\n",
    "    img = img.resize((299, 299), resample=Resampling.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56922062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for image preprocessing\n",
    "def preprocess_input(x):\n",
    "    x /= 127.5\n",
    "    x -= 1.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92b8ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.11372548, -0.15294117, -0.19999999],\n",
       "         [-0.11372548, -0.15294117, -0.19999999],\n",
       "         [-0.10588235, -0.14509803, -0.19215685],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.04313725, -0.04313725, -0.10588235],\n",
       "         [-0.11372548, -0.11372548, -0.17647058]],\n",
       "\n",
       "        [[-0.09019607, -0.12941176, -0.17647058],\n",
       "         [-0.09019607, -0.12941176, -0.17647058],\n",
       "         [-0.08235294, -0.12156862, -0.16862744],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.04313725, -0.04313725, -0.10588235],\n",
       "         [-0.10588235, -0.10588235, -0.16862744]],\n",
       "\n",
       "        [[-0.09803921, -0.1372549 , -0.18431371],\n",
       "         [-0.09803921, -0.1372549 , -0.18431371],\n",
       "         [-0.09019607, -0.12941176, -0.17647058],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.03529412, -0.03529412, -0.09803921],\n",
       "         [-0.09019607, -0.09019607, -0.15294117]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.67058825, -0.7019608 , -0.7254902 ],\n",
       "         [-0.6862745 , -0.7176471 , -0.7411765 ],\n",
       "         [-0.70980394, -0.7411765 , -0.7647059 ],\n",
       "         ...,\n",
       "         [-0.62352943, -0.84313726, -0.9529412 ],\n",
       "         [-0.6313726 , -0.8509804 , -0.9607843 ],\n",
       "         [-0.6392157 , -0.85882354, -0.96862745]],\n",
       "\n",
       "        [[-0.52156866, -0.5529412 , -0.5764706 ],\n",
       "         [-0.52156866, -0.5529412 , -0.5764706 ],\n",
       "         [-0.5137255 , -0.54509807, -0.5686275 ],\n",
       "         ...,\n",
       "         [-0.5921569 , -0.8117647 , -0.92156863],\n",
       "         [-0.6       , -0.81960785, -0.92941177],\n",
       "         [-0.60784316, -0.827451  , -0.9372549 ]],\n",
       "\n",
       "        [[-0.62352943, -0.654902  , -0.6784314 ],\n",
       "         [-0.6156863 , -0.64705884, -0.67058825],\n",
       "         [-0.60784316, -0.6392157 , -0.6627451 ],\n",
       "         ...,\n",
       "         [-0.5686275 , -0.79607844, -0.90588236],\n",
       "         [-0.5764706 , -0.8039216 , -0.9137255 ],\n",
       "         [-0.58431375, -0.8117647 , -0.92156863]]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the image\n",
    "x = np.array(img, dtype='float32') # must be float type\n",
    "X = np.array([x])\n",
    "X = preprocess_input(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98be7404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.4526663, -7.4974055, -4.0071616, -4.0869083, 11.95938  ,\n",
       "       -3.2584872, -4.643537 ,  1.930927 , -5.2949286, -4.09361  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on image using tf-lite model\n",
    "interpreter.set_tensor(input_index, X)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b419e087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -4.4526663,\n",
       " 'hat': -7.4974055,\n",
       " 'longsleeve': -4.0071616,\n",
       " 'outwear': -4.0869083,\n",
       " 'pants': 11.95938,\n",
       " 'shirt': -3.2584872,\n",
       " 'shoes': -4.643537,\n",
       " 'shorts': 1.930927,\n",
       " 'skirt': -5.2949286,\n",
       " 't-shirt': -4.09361}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check predictions\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748dc2c",
   "metadata": {},
   "source": [
    "## Simpler Way of Loading and Preprocessing Image\n",
    "\n",
    "There is even simpler way of loading and preprocessing image using `keras-image-helper` from the project [here](https://github.com/alexeygrigorev/keras-image-helper).\n",
    "\n",
    "In order to use keras-image-helper library we need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73462a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-image-helper\n",
      "  Downloading keras_image_helper-0.0.1-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: numpy in d:\\repos\\ml-engineering\\env\\lib\\site-packages (from keras-image-helper) (1.23.4)\n",
      "Requirement already satisfied: pillow in d:\\repos\\ml-engineering\\env\\lib\\site-packages (from keras-image-helper) (9.2.0)\n",
      "Installing collected packages: keras-image-helper\n",
      "Successfully installed keras-image-helper-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdcca0",
   "metadata": {},
   "source": [
    "Our model needs tf-lite from tensorflow package, however, we can use tf-lite without depending on tensorflow and this is what we are looking for. For this, we need to install `tflite-runtime` from [GitHub Coral Python](https://github.com/google-coral/py-repo) page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39c3108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://google-coral.github.io/py-repo/\n",
      "Collecting tflite_runtime\n",
      "  Downloading https://github.com/google-coral/pycoral/releases/download/v2.0.0/tflite_runtime-2.5.0.post1-cp39-cp39-win_amd64.whl (867 kB)\n",
      "     ------------------------------------ 867.1/867.1 kB 869.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in d:\\repos\\ml-engineering\\env\\lib\\site-packages (from tflite_runtime) (1.23.4)\n",
      "Installing collected packages: tflite_runtime\n",
      "Successfully installed tflite_runtime-2.5.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f1b3f",
   "metadata": {},
   "source": [
    "Now, instead of importing `import tensorflow.lite as tflite` which comes from tensorflow, we only want to import `import tflite_runtime.interpreter as tflite` from the tflite_runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9cfafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite\n",
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize interpreter\n",
    "interpreter = tflite.Interpreter(model_path='clothing-model.tflite')\n",
    "# Allocate memory\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input index\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "# Get output index\n",
    "output_index = interpreter.get_output_details()[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d6f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor for the model we used for training (Xception)\n",
    "preprocessor = create_preprocessor('xception', target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dea9f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.11372548, -0.15294117, -0.19999999],\n",
       "         [-0.11372548, -0.15294117, -0.19999999],\n",
       "         [-0.10588235, -0.14509803, -0.19215685],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.04313725, -0.04313725, -0.10588235],\n",
       "         [-0.11372548, -0.11372548, -0.17647058]],\n",
       "\n",
       "        [[-0.09019607, -0.12941176, -0.17647058],\n",
       "         [-0.09019607, -0.12941176, -0.17647058],\n",
       "         [-0.08235294, -0.12156862, -0.16862744],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.04313725, -0.04313725, -0.10588235],\n",
       "         [-0.10588235, -0.10588235, -0.16862744]],\n",
       "\n",
       "        [[-0.09803921, -0.1372549 , -0.18431371],\n",
       "         [-0.09803921, -0.1372549 , -0.18431371],\n",
       "         [-0.09019607, -0.12941176, -0.17647058],\n",
       "         ...,\n",
       "         [-0.01960784, -0.01960784, -0.08235294],\n",
       "         [-0.03529412, -0.03529412, -0.09803921],\n",
       "         [-0.09019607, -0.09019607, -0.15294117]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.67058825, -0.7019608 , -0.7254902 ],\n",
       "         [-0.6862745 , -0.7176471 , -0.7411765 ],\n",
       "         [-0.70980394, -0.7411765 , -0.7647059 ],\n",
       "         ...,\n",
       "         [-0.62352943, -0.84313726, -0.9529412 ],\n",
       "         [-0.6313726 , -0.8509804 , -0.9607843 ],\n",
       "         [-0.6392157 , -0.85882354, -0.96862745]],\n",
       "\n",
       "        [[-0.52156866, -0.5529412 , -0.5764706 ],\n",
       "         [-0.52156866, -0.5529412 , -0.5764706 ],\n",
       "         [-0.5137255 , -0.54509807, -0.5686275 ],\n",
       "         ...,\n",
       "         [-0.5921569 , -0.8117647 , -0.92156863],\n",
       "         [-0.6       , -0.81960785, -0.92941177],\n",
       "         [-0.60784316, -0.827451  , -0.9372549 ]],\n",
       "\n",
       "        [[-0.62352943, -0.654902  , -0.6784314 ],\n",
       "         [-0.6156863 , -0.64705884, -0.67058825],\n",
       "         [-0.60784316, -0.6392157 , -0.6627451 ],\n",
       "         ...,\n",
       "         [-0.5686275 , -0.79607844, -0.90588236],\n",
       "         [-0.5764706 , -0.8039216 , -0.9137255 ],\n",
       "         [-0.58431375, -0.8117647 , -0.92156863]]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image url for making prediction\n",
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "\n",
    "# Preprocess the image\n",
    "X = preprocessor.from_url(url) # there is also a method called 'from_path'\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751492fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.452664 , -7.497402 , -4.0071664, -4.0869126, 11.959379 ,\n",
       "       -3.258485 , -4.643536 ,  1.9309283, -5.2949286, -4.0936103],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on image like befor\n",
    "interpreter.set_tensor(input_index, X)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)[0]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45d648f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -4.452664,\n",
       " 'hat': -7.497402,\n",
       " 'longsleeve': -4.0071664,\n",
       " 'outwear': -4.0869126,\n",
       " 'pants': 11.959379,\n",
       " 'shirt': -3.258485,\n",
       " 'shoes': -4.643536,\n",
       " 'shorts': 1.9309283,\n",
       " 'skirt': -5.2949286,\n",
       " 't-shirt': -4.0936103}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
